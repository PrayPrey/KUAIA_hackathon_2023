{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, Subset\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "import cufflinks as cf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cf.go_offline(connected = True)\n",
    "\n",
    "\n",
    "from mrcnn.visualize import display_instances\n",
    "from mrcnn import visualize\n",
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from mrcnn.config import Config\n",
    "from mrcnn.model import MaskRCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from mrcnn.visualize import * \n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "import math\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as KL\n",
    "import keras.engine as KE\n",
    "import keras.models as KM\n",
    "import keras.callbacks\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import Image\n",
    "import skimage \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mrcnn import utils\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_layers = nn.Sequential(nn.Linear(input_dim, 200),\n",
    "                                          nn.LeakyReLU(0.1),\n",
    "                                          nn.Linear(200,100),\n",
    "                                          nn.LeakyReLU(0.1),\n",
    "                                          nn.Linear(100,50),\n",
    "                                          nn.LeakyReLU(0.1),\n",
    "                                          nn.Linear(50,20),\n",
    "                                          nn.LeakyReLU(0.1),\n",
    "                                          nn.Linear(20,5),\n",
    "                                          nn.LeakyReLU(0.1),\n",
    "                                          nn.Linear(5, output_dim), \n",
    "                                           # 최종 결과는 (20, 3) 이 되므로, dim=-1 로 label 확률값이 들어 있는 마지막 차원을 지정해줘야 함\n",
    "                                          nn.LogSoftmax(dim = -1)\n",
    "                                          )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.linear_layers(x)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_lst =[\n",
    "    \"short sleeve top\",\n",
    "    \"long sleeve top\",\n",
    "    \"short sleeve outwear\",\n",
    "    \"long sleeve outwear\",\n",
    "    \"vest\",\n",
    "    \"sling\",\n",
    "    \"shorts\",\n",
    "    \"trousers\",\n",
    "    \"skirt\",\n",
    "    \"short sleeve dress\",\n",
    "    \"long sleeve dress\",\n",
    "    \"vest dress\",\n",
    "    \"sling dress\",\n",
    "    'skirt_2'\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(13, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items={'short sleeve top': 0, 'long sleeve top': 1, 'short sleeve outwear':2,\n",
    "       'long sleeve outwear': 3,'vest':4, 'sling':5, 'shorts':6, 'trousers':7,\n",
    "       'skirt':8, 'short sleeve dress':9, 'long sleeve dress':10,'vest dress':11,'sling dress':12,'skirt_2' : 13}\n",
    "items\n",
    "\n",
    "items_2 = np.array(['아메리칸 캐주얼', '캐주얼', '댄디', '포멀', '걸리시', '골프', '홈웨어', '레트로', '로맨틱',\n",
    "       '스포츠', '스트릿', '시크'])\n",
    "\n",
    "col = items_2[[8 , 9,  2, 10,  0,  1, 11,  3,  4,  6,  5,  7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'short sleeve top': 0,\n",
       " 'long sleeve top': 1,\n",
       " 'short sleeve outwear': 2,\n",
       " 'long sleeve outwear': 3,\n",
       " 'vest': 4,\n",
       " 'sling': 5,\n",
       " 'shorts': 6,\n",
       " 'trousers': 7,\n",
       " 'skirt': 8,\n",
       " 'short sleeve dress': 9,\n",
       " 'long sleeve dress': 10,\n",
       " 'vest dress': 11,\n",
       " 'sling dress': 12,\n",
       " 'skirt_2': 13}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:02<00:00, 27.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import tqdm\n",
    "\n",
    "lst_name = []\n",
    "lst_influ_total = []\n",
    "\n",
    "directory = 'Total_baseline_data/인플루언서정보_샘플2/'\n",
    "for files in tqdm.tqdm(os.listdir(directory)):\n",
    "    \n",
    "    lst_influ_all = []\n",
    "\n",
    "    \n",
    "    for file in os.listdir(directory + files):\n",
    "        lst_influ = []\n",
    "        if 'csv' in file:\n",
    "            df = pd.read_csv(directory + files + '/' + file, index_col=0)\n",
    "            \n",
    "            df2 = df.copy()\n",
    "\n",
    "            df3 = df2[df2['class_name'] != 'Not_Detected'].copy()\n",
    "\n",
    "            df3['new_class'] = [items[i] if i!= 'skirt_2' else 8 for i in df3.class_name ]\n",
    "\n",
    "            df3['file_name'] = df3['file_name'] \n",
    "\n",
    "            df3['file_name'] = [i.split('_')[0]  for i in df3['file_name'] ]\n",
    "\n",
    "            df3.index.name = 'index'\n",
    "\n",
    "            A = []\n",
    "            for idx,i in enumerate(np.unique(df3.file_name)):\n",
    "                A.append(df3[df3['file_name'] == i])\n",
    "\n",
    "\n",
    "            df4 = df3.drop_duplicates('file_name').copy()\n",
    "\n",
    "            df_zero = pd.DataFrame(np.zeros((df4.shape[0], 13)))\n",
    "            \n",
    "            for aa in A:\n",
    "                data = aa[['score','new_class']].values\n",
    "            \n",
    "                for k in data:\n",
    "                    lst = [0] * 13\n",
    "                    lst[int(k[1])] = k[0]\n",
    "                    lst_influ.append(lst)\n",
    "                    lst_influ_all.append(torch.exp(model(torch.Tensor(lst))).detach().numpy())\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    \n",
    "    lst_influ_total.append(pd.DataFrame(lst_influ_all).sum(axis=0).values)\n",
    "    lst_name.append(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.DataFrame(lst_influ_total)\n",
    "df_total.index = lst_name\n",
    "df_total.columns = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.to_excel('style_influencer.xlsx',encoding = 'utf-8-isg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 12)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# 모델 생성\n",
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "\n",
    "model_bert = BertModel.from_pretrained(CHECKPOINT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_pretrained, dropout_rate=0.5):\n",
    "        # 부모클래스 초기화\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        # 사전학습 모델 지정\n",
    "        self.bert = BertModel.from_pretrained(bert_pretrained)\n",
    "        # dropout 설정\n",
    "        self.dr = nn.Dropout(p=dropout_rate)\n",
    "        # 최종 출력층 정의\n",
    "        self.fc = nn.Linear(768, 12)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 입력을 pre-trained bert model 로 대입\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        # 결과의 last_hidden_state 가져옴\n",
    "        last_hidden_state = output['last_hidden_state']\n",
    "        # last_hidden_state[:, 0, :]는 [CLS] 토큰을 가져옴\n",
    "        x = self.dr(last_hidden_state[:, 0, :])\n",
    "        # FC 을 거쳐 최종 출력\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = CustomBertModel(CHECKPOINT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = bert(A['input_ids'].reshape(1,-1), A['attention_mask'].reshape(1,-1), A['token_type_ids'].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.5197], grad_fn=<MaxBackward0>),\n",
       "indices=tensor([11]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_NAME = 'kykim/bert-kor-base'\n",
    "tokenizer_pretrained = CHECKPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence,tokenizer):\n",
    "\n",
    "        # 토큰화 처리\n",
    "        tokens = tokenizer(\n",
    "            sentence,                # 1개 문장 \n",
    "            return_tensors='pt',     # 텐서로 반환\n",
    "            truncation=True,         # 잘라내기 적용\n",
    "            padding='max_length',    # 패딩 적용\n",
    "            add_special_tokens=True  # 스페셜 토큰 적용\n",
    "        )\n",
    "\n",
    "        input_ids = tokens['input_ids'].squeeze(0)           # 2D -> 1D\n",
    "        attention_mask = tokens['attention_mask'].squeeze(0) # 2D -> 1D\n",
    "        token_type_ids = torch.zeros_like(attention_mask)\n",
    "\n",
    "        # input_ids, attention_mask, token_type_ids 이렇게 3가지 요소를 반환하도록 합니다.\n",
    "        # input_ids: 토큰\n",
    "        # attention_mask: 실제 단어가 존재하면 1, 패딩이면 0 (패딩은 0이 아닐 수 있습니다)\n",
    "        # token_type_ids: 문장을 구분하는 id. 단일 문장인 경우에는 전부 0\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask, \n",
    "            'token_type_ids': token_type_ids }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert.load_state_dict(torch.load('bert-kor-base.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원하는 스타일을 입력하시오 : 안녀앟세요\n",
      "해당되는 스타일은 시크 입니다 \n",
      "\n",
      "#################인플루언서 순위#####################\n",
      "\t1순위 : @sualboys\n",
      "\t2순위 : @yangkoon__dl\n",
      "\t3순위 : @odor_bubu\n",
      "\t4순위 : @mavlfit\n",
      "\t5순위 : @zxcvr0626\n",
      "\t6순위 : @kj_m.w\n",
      "\t7순위 : @jindalorian\n",
      "\t8순위 : @lil_0uzi_vert\n",
      "\t9순위 : @uuuuk_2_\n",
      "\t10순위 : @so_h_appy\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "sentence = input('원하는 스타일을 입력하시오 : ')\n",
    "\n",
    "A = preprocessing(sentence= sentence,tokenizer=tokenizer)\n",
    "B = bert(A['input_ids'].reshape(1,-1), A['attention_mask'].reshape(1,-1), A['token_type_ids'].reshape(1,-1))\n",
    "C = B.max(dim=1)\n",
    "style = col[C[1].detach().numpy()][0]\n",
    "print('해당되는 스타일은 ' + style +' 입니다','\\n')\n",
    "\n",
    "print('#################인플루언서 순위#####################')\n",
    "lst_img = []\n",
    "for idx, i in enumerate(df_total[style].sort_values(ascending= False).index[0:10]):\n",
    "    lst_img.append(i)\n",
    "    print('\\t' +str(idx+1) + '순위 : ' + i)\n",
    "    \n",
    "num = np.random.choice(range(len(file_lst)))\n",
    "file_lst = os.listdir('Total_baseline_data/인플루언서정보_샘플2/' + lst_img[0] + '/images')\n",
    "\n",
    "num_2 = 0\n",
    "for idx,i in enumerate(file_lst):\n",
    "    if ('converted' in i):\n",
    "        num_2 += 1\n",
    "        if (num_2 == num):\n",
    "            img = Image.open('Total_baseline_data/인플루언서정보_샘플2/' + lst_img[0] + '/images/'+i  )\n",
    "            img.show()\n",
    "            print(1)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('Total_baseline_data/인플루언서정보_샘플2/' + lst_img[0] + '/images/'+i  )\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_baseline_data\\인플루언서정보_샘플1\\@hotneul\\images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3007463791760602227_0_converted.jpg\n"
     ]
    }
   ],
   "source": [
    "num = np.random.choice(range(len(file_lst)))\n",
    "file_lst = os.listdir('Total_baseline_data/인플루언서정보_샘플2/' + lst_img[-1] + '/images')\n",
    "\n",
    "num_2 = 0\n",
    "for idx,i in enumerate(file_lst):\n",
    "    if ('converted' in i):\n",
    "        num_2 += 1\n",
    "        if (num_2 == num):\n",
    "            print(i)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 로맨틱\n",
      "1 스포츠\n",
      "2 댄디\n",
      "3 스트릿\n",
      "4 아메리칸 캐주얼\n",
      "5 캐주얼\n",
      "6 시크\n",
      "7 포멀\n",
      "8 걸리시\n",
      "9 홈웨어\n",
      "10 골프\n",
      "11 레트로\n"
     ]
    }
   ],
   "source": [
    "for _, i in enumerate(col):\n",
    "    print(_,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 아메리칸 캐주얼\n",
      "1 캐주얼\n",
      "2 댄디\n",
      "3 포멀\n",
      "4 걸리시\n",
      "5 골프\n",
      "6 홈웨어\n",
      "7 레트로\n",
      "8 로맨틱\n",
      "9 스포츠\n",
      "10 스트릿\n",
      "11 시크\n"
     ]
    }
   ],
   "source": [
    "for _,i in enumerate(items_2):\n",
    "    print(_,i)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
